{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def removeStopwords(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    ps = PorterStemmer()\n",
    "    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_test = sorted([join('sampled_test', f) for f in listdir('sampled_test') if isfile(join('sampled_test', f))])\n",
    "sampled_train = sorted([join('sampled_train', f) for f in listdir('sampled_train') if isfile(join('sampled_train', f))])\n",
    "\n",
    "d = pd.read_csv('annotations_metadata.csv')\n",
    "d = np.array(d.iloc[:, [0,4]])\n",
    "dic = {}\n",
    "for i in d:\n",
    "    dic[i[0]] = i[1]\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for txt in sampled_train:\n",
    "    txt_id = txt.split('\\\\')[-1][:-4]\n",
    "    with open(txt, encoding=\"utf8\") as f:\n",
    "        text = str(f.read())\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.split()\n",
    "        ps = PorterStemmer()\n",
    "        text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "        if len(text)!=0:\n",
    "            X_train.append(\" \".join(text))\n",
    "            y_train.append(dic[txt_id])\n",
    "\n",
    "for txt in sampled_test:\n",
    "    txt_id = txt.split('\\\\')[-1][:-4]\n",
    "    with open(txt, encoding=\"utf8\") as f:\n",
    "        text = str(f.read()).lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.split()\n",
    "        ps = PorterStemmer()\n",
    "        text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "        if len(text)!=0:\n",
    "            X_test.append(\" \".join(text))\n",
    "            y_test.append(dic[txt_id])\n",
    "\n",
    "text = str(\" \".join(X_train)) + \" \" + str(\" \".join(X_test))\n",
    "corpus = list(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=corpus)\n",
    "\n",
    "# Tfidf Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(vocabulary=corpus)\n",
    "\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "X = vectorizer.transform(X_test)\n",
    "X_test = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_train = labelencoder_y.fit_transform(y_train)\n",
    "y_test = labelencoder_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=50, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty='l1',max_iter=50,solver='liblinear')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7109704641350211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.21197962]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=500, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty='l1', random_state=0, shuffle=True, tol=None,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(penalty='l1',alpha=0.0001,max_iter=500,random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6814345991561181\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.65 0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords\n",
    "\n",
    "X_train1 = []\n",
    "X_test1 = []\n",
    "y_train1 = []\n",
    "y_test1 = []\n",
    "\n",
    "for txt in sampled_train:\n",
    "    txt_id = txt.split('\\\\')[-1][:-4]\n",
    "    with open(txt, encoding=\"utf8\") as f:\n",
    "        text = str(f.read())\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.split()\n",
    "        ps = PorterStemmer()\n",
    "        if len(text)!=0:\n",
    "            X_train1.append(\" \".join(text))\n",
    "            y_train1.append(dic[txt_id])\n",
    "\n",
    "for txt in sampled_test:\n",
    "    txt_id = txt.split('\\\\')[-1][:-4]\n",
    "    with open(txt, encoding=\"utf8\") as f:\n",
    "        text = str(f.read()).lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.split()\n",
    "        ps = PorterStemmer()\n",
    "        if len(text)!=0:\n",
    "            X_test1.append(\" \".join(text))\n",
    "            y_test1.append(dic[txt_id])\n",
    "\n",
    "text = str(\" \".join(X_train1)) + \" \" + str(\" \".join(X_test1))\n",
    "corpus1 = list(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "# vectorizer1 = CountVectorizer(vocabulary=corpus1)\n",
    "\n",
    "# Tfid fVectorizer\n",
    "vectorizer1 = TfidfVectorizer(vocabulary=corpus1)\n",
    "X = vectorizer1.fit_transform(X_train1)\n",
    "X_train1 = pd.DataFrame(data=X.toarray(), columns=vectorizer1.get_feature_names())\n",
    "\n",
    "X = vectorizer1.transform(X_test1)\n",
    "X_test1 = pd.DataFrame(data=X.toarray(), columns=vectorizer1.get_feature_names())\n",
    "\n",
    "y_train1 = labelencoder_y.fit_transform(y_train1)\n",
    "y_test1 = labelencoder_y.fit_transform(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7557894736842106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "classifier1 = LogisticRegression()\n",
    "classifier1.fit(X_train1, y_train1)\n",
    "y_pred1 = classifier1.predict(X_test1)\n",
    "print(\"Accuracy :\", accuracy_score(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6926315789473684\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "clf1 = Perceptron()\n",
    "clf1.fit(X_train1, y_train1)\n",
    "pred1 = clf1.predict(X_test1)\n",
    "print(\"Accuracy :\", accuracy_score(y_test1, pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
